---
title: "Machine Learning Notes 1"
date: 2025-07-02 20:10:00 +0800
math: true
categories: [Machine Learning]
tags: [AI]
---

Triple post OMG

# Support Vector Machines (SVM) — Comprehensive Lecture Notes

These notes (~2,200 words) take you from zero to deep intuition on SVM, covering history, math, algorithms, kernels, and applications. Enjoy!!

---

## 1. Title & Introduction

**Support Vector Machines (SVM)** are a family of supervised learning algorithms for binary classification (and regression) that seek the **maximum‐margin** decision boundary between classes. Early ideas date to the **1960s** (Vapnik & Lerner), but SVMs were fully formalized in the **1990s** by Vladimir Vapnik and colleagues, grounded in statistical learning theory (VC dimension).

> **Why margin matters:** A larger margin—the distance between the boundary and the nearest training points—yields lower generalization error on unseen data.

**Key applications:**  
- **Text categorization** (spam filtering, news grouping)  
- **Handwriting recognition** (MNIST digit classification)  
- **Satellite imagery** (land‐cover mapping)  
- **Sentiment analysis** (positive/negative reviews)  
- **Bioinformatics** (gene/protein classification)

---

## 2. Decision Boundaries & Hyperplanes

### 2.1 What is a hyperplane?

In $$n$$‑dimensional feature space, a hyperplane is the set of points $$x\in\mathbb{R}^n$$ satisfying  
$$
w^\top x + b = 0.
$$  
- $$w = [w_1,\dots,w_n]^\top\in\mathbb{R}^n$$ is the **weight vector** (normal to the hyperplane).  
- $$x = [x_1,\dots,x_n]^\top\in\mathbb{R}^n$$ is any **feature vector**.  
- $$b\in\mathbb{R}$$ is the **bias** (offset).

| Data dimension | Hyperplane | Boundary dimension |
|----------------|------------|---------------------|
| 1D             | Point      | 0D                  |
| 2D             | Line       | 1D                  |
| 3D             | Plane      | 2D                  |

### 2.2 Classification rule

Given a new point $$x$$, prediction is  
$$
\hat y = \operatorname{sign}(w^\top x + b),
$$  
where  
- $$\hat y\in\{+1,-1\}$$ is the **predicted label**.  

> **Interpretation:**  
> - If $$w^\top x + b>0$$ ⇒ class +1  
> - If $$w^\top x + b<0$$ ⇒ class –1  

### 2.3 Geometric margin

The (signed) distance from $$x$$ to the hyperplane is  
$$
\gamma(x) = \frac{w^\top x + b}{\|w\|},
$$  
where  
- $$\|w\| = \sqrt{w^\top w}$$ is the Euclidean norm of $$w$$.  

The **margin** of a classifier on dataset $$\{(x_i,y_i)\}_{i=1}^m$$ is  
$$
\min_{i=1,\dots,m} \bigl|\gamma(x_i)\bigr|.
$$  
**Goal:** maximize this minimum distance.

---

## 3. Support Vectors

### 3.1 Definition

**Support vectors** are the training points **closest** to the hyperplane—i.e. those for which the margin equality holds:  
$$
y_i\,(w^\top x_i + b) = 1,
$$  
with  
- $$x_i$$ the $$i$$‑th training feature vector,  
- $$y_i\in\{+1,-1\}$$ its true label.  

### 3.2 Lagrange multipliers & KKT

To enforce margin constraints, introduce **Lagrange multipliers** $$\alpha_i\ge0$$ and form the **Lagrangian**. KKT (Karush–Kuhn–Tucker) conditions yield:  

1. **Primal–dual relation:**  
   $$
   w = \sum_{i=1}^m \alpha_i\,y_i\,x_i.
   $$

2. **Balance condition:**  
   $$
   \sum_{i=1}^m \alpha_i\,y_i = 0.
   $$

3. **Complementary slackness:**  
   $$
   \alpha_i\bigl[y_i(w^\top x_i + b) - 1\bigr] = 0.
   $$

> **Tug‑of‑war analogy:** Each support vector "pulls" on the hyperplane with weight $$\alpha_i$$; the balance condition ensures equal total pull from +1 and –1 classes.

---

## 4. Hard‑Margin SVM

### 4.1 Primal formulation

Assumes data is linearly separable. Solve  
$$
\begin{aligned}
& \min_{w,b}\quad \frac12\|w\|^2,\\
& \text{s.t.}\quad y_i\,(w^\top x_i + b)\;\ge\;1,\quad i=1,\dots,m.
\end{aligned}
$$  
- **Objective** $$\tfrac12\|w\|^2$$ enforces a wide margin.  
- **Constraints** keep all points outside the margin.

### 4.2 Lagrangian & KKT → Dual

Form Lagrangian  
$$
L(w,b,\alpha) = \frac12\|w\|^2 - \sum_{i=1}^m \alpha_i\bigl[y_i(w^\top x_i + b)-1\bigr].
$$  
KKT yields the dual problem:  
$$
\begin{aligned}
& \max_{\alpha\ge0}\quad
\sum_{i=1}^m \alpha_i \;-\;\frac12\sum_{i,j=1}^m \alpha_i\alpha_j\,y_i y_j\,(x_i^\top x_j),\\
& \text{s.t.}\quad
\sum_{i=1}^m \alpha_i\,y_i = 0.
\end{aligned}
$$  
- **Variables:** $$\alpha_i\ge0$$ for $$i=1,\dots,m$$.  
- **Objective:** trades total support weight vs. similarity penalty.  
- **Constraint:** ensures the hyperplane is unbiased.

---

## 5. Soft‑Margin SVM

Real data can overlap. Introduce **slack variables** $$\xi_i\ge0$$ to allow margin violations:

### 5.1 Primal with slack

$$
\begin{aligned}
& \min_{w,b,\xi}\;
\frac12\|w\|^2 + C\sum_{i=1}^m \xi_i,\\
& \text{s.t.}\quad
y_i(w^\top x_i + b)\;\ge\;1 - \xi_i,\quad
\xi_i\ge0,\quad i=1,\dots,m.
\end{aligned}
$$  
- $$\xi_i$$ measures how far $$x_i$$ falls inside the margin (or misclassified if $$\xi_i>1$$).  
- $$C>0$$ is the **regularization parameter** controlling the trade‑off.

### 5.2 Role of $$C$$

| $$C$$ large                            | $$C$$ small                             |
|----------------------------------------|-----------------------------------------|
| Heavy penalty on slack (few violations)| Slack cheap (more violations allowed)   |
| Narrow margin, fits data closely       | Wider margin, better generalization     |

### 5.3 Soft‑margin dual

The dual becomes (with upper bounds $$0\le\alpha_i\le C$$):  
$$
\begin{aligned}
& \max_{0\le\alpha_i\le C}\quad
\sum_{i=1}^m \alpha_i \;-\;\frac12\sum_{i,j=1}^m \alpha_i\alpha_j\,y_i y_j\,(x_i^\top x_j),\\
& \text{s.t.}\quad
\sum_{i=1}^m \alpha_i\,y_i = 0.
\end{aligned}
$$

---

## 6. Kernel Functions & Non‑linear SVM

### 6.1 Why kernels?

When data isn’t separable in input space (e.g., concentric circles), we map $$x$$ into a **feature space** via  
$$
\phi(x): \mathbb{R}^n \to \mathbb{R}^p,
$$  
where separation is possible.

### 6.2 Kernel trick

Define  
$$
K(u,v) = \phi(u)^\top \phi(v).
$$  
In the dual, replace all $$x_i^\top x_j$$ with $$K(x_i,x_j)$$, avoiding explicit $$\phi$$.

### 6.3 Dual decision function

Once $$\alpha$$ are found, predict via  
$$
f(x) = \operatorname{sign}\Bigl(\sum_{i=1}^m \alpha_i\,y_i\,K(x_i,x) + b\Bigr).
$$

### 6.4 Common kernels

1. **Linear**  
   $$
   K(u,v) = u^\top v + 1.
   $$

2. **Polynomial** (degree $$d$$)  
   $$
   K(u,v) = (u^\top v + c)^d,\quad c\ge0.
   $$  

3. **RBF (Gaussian)**  
   $$
   K(u,v) = \exp\bigl(-\gamma\|u-v\|^2\bigr),\quad\gamma>0.
   $$  

4. **Sigmoid**  
   $$
   K(u,v) = \tanh(\alpha\,u^\top v + c).
   $$  

---

## 7. Mathematical Implementation & Algorithm

### 7.1 Preprocessing

- **Center** each feature to zero mean.  
- **Scale** to unit variance (important for RBF).  

### 7.2 Hyperparameter selection

- Choose kernel type and parameters:  
  - $$C$$ (soft‑margin penalty)  
  - $$d,c$$ for polynomial  
  - $$\gamma$$ for RBF  
  - $$\alpha,c$$ for sigmoid  
- Use **grid search** + **k‑fold cross‑validation**.

### 7.3 Solver

- Solve the **dual QP** (e.g., via SMO algorithm).  
- Obtain optimal $$\alpha_i$$.

### 7.4 Compute model parameters

- For **linear** kernel:  
  $$
  w = \sum_{i=1}^m \alpha_i\,y_i\,x_i.
  $$  
- Compute bias $$b$$ by averaging over any support vector $$k$$ with $$0<\alpha_k<C$$:  
  $$
  b = y_k - \sum_{i=1}^m \alpha_i\,y_i\,K(x_i,x_k).
  $$

### 7.5 Prediction

Given new $$x$$,  
$$
\hat y = \operatorname{sign}\Bigl(\sum_{i=1}^m \alpha_i\,y_i\,K(x_i,x) + b\Bigr).
$$

### 7.6 Complexity

- Training: $$O(m^2)$$ to $$O(m^3)$$ in number of samples.  
- Prediction: $$O(\#SV)$$ per test point.

---

## 8. Practical Tips & Applications

- **Text categorization:** linear SVM on TF‑IDF; sparse data favors linear.  
- **Handwritten digits:** RBF on pixel intensities for non‑linear boundaries.  
- **Satellite classification:** polynomial on spectral bands to capture interactions.  
- **Sentiment analysis:** word‑embedding vectors + RBF or sigmoid.  
- **Imbalanced classes:** weight $$C$$ per class or use class weights.  
- **Outlier care:** scale $$C$$ down to tolerate noise.

---

## 9. Four Mini Case Studies

1. **Spam filter (linear):**  
   - Features: word counts → TF‑IDF.  
   - Kernel: linear.  

2. **Digit recognition (RBF):**  
   - Features: raw pixels.  
   - Kernel: RBF with tuned $$\gamma$$.  

3. **Satellite bands (polynomial):**  
   - Features: multispectral intensities.  
   - Kernel: polynomial degree 2 or 3.  

4. **Sentiment (embedding + kernel):**  
   - Features: word2vec averages.  
   - Kernel: sigmoid or RBF.

---

## 10. Summary & Further Reading

- **Key equations:**  
  - Hyperplane: $$w^\top x + b = 0$$  
  - Margin: $$\gamma(x) = \frac{w^\top x + b}{\|w\|}$$  
  - Primal (hard): $$\min\tfrac12\|w\|^2,\;y_i(w^\top x_i + b)\ge1$$  
  - Primal (soft): $$\min\tfrac12\|w\|^2 + C\sum\xi_i$$  
  - Dual: $$\max\sum\alpha_i - \tfrac12\sum_{i,j}\alpha_i\alpha_j y_i y_j K(x_i,x_j)$$  
  - Decision: $$\hat y=\operatorname{sign}\bigl(\sum\alpha_i y_i K(x_i,x)+b\bigr)$$  

- **Further reading:**  
  - Vapnik, “The Nature of Statistical Learning Theory”  
  - Cortes & Vapnik, “Support‐Vector Networks” (1995)  
  - scikit‑learn SVM docs: [link](https://scikit-learn.org/stable/modules/svm.html)
  - Scholkopf & Smola, “Learning with Kernels”  


# I  _love_  LLM tools — they ease the pain of sitting through a 1.5-hour lecture like nothing else.
PS:
Why does the camp get exponentially harder with each class? Feels like _both_ my math skills and coding skills are getting _discriminated again_, lol. No wonder the smartest minds on Earth are the ones who created all the ML and DL breakthroughs!<br>
~~Do I actually need to be learning 12/7 to actually keep up with the world? Ts is soooo cooked...~~

